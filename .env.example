# --- LLM Configuration ---
# Primary provider (openai, anthropic, gemini, groq)
KATALYST_LITELLM_PROVIDER=openai

# Option 1: Use pre-configured provider profiles (recommended for easy switching)
# KATALYST_LLM_PROFILE=anthropic  # Uses Claude models with sensible defaults
# KATALYST_LLM_PROFILE=gemini     # Uses Gemini models with sensible defaults
# KATALYST_LLM_PROFILE=groq       # Uses Groq/Llama models with sensible defaults

# Option 2: Manually specify models (advanced users)
# Use a powerful model for planning and replanning (high-reasoning tasks)
KATALYST_REASONING_MODEL="gpt-4.1"

# Use a faster, cheaper model for execution and tool use (low-reasoning tasks)
KATALYST_EXECUTION_MODEL="gpt-4.1"

# Fallback model if primary model fails
KATALYST_LLM_MODEL_FALLBACK="gpt-4o"

# Default timeout for all LLM calls
KATALYST_LITELLM_TIMEOUT=45

# --- Agent Behavior ---
KATALYST_AUTO_APPROVE=False
KATALYST_MAX_INNER_REACT_CYCLES=20
KATALYST_MAX_OUTER_PLANNER_CYCLES=10
KATALYST_RECURSION_LIMIT=250

# --- Chat History Management ---
# Trigger chat summarization when history exceeds this many messages
KATALYST_CHAT_SUMMARY_TRIGGER=50
# Number of recent messages to keep after summarization
KATALYST_CHAT_SUMMARY_KEEP_LAST_N=10

# --- Context Tracking ---
# Number of recent file operations to track
KATALYST_FILE_CONTEXT_HISTORY=10
# Number of recent tool operations to track
KATALYST_OPERATIONS_CONTEXT_HISTORY=10

# --- Logging Configuration ---
# Suppress litellm debug messages (set to WARNING, ERROR, or CRITICAL)
LITELLM_LOG=WARNING

# --- API Keys ---
OPENAI_API_KEY=