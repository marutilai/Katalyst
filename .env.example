# --- LLM Configuration ---
# Primary provider (openai, anthropic, ollama, groq, together)
KATALYST_LLM_PROVIDER=openai

# Option 1: Use pre-configured provider profiles (recommended for easy switching)
# KATALYST_LLM_PROFILE=anthropic  # Uses Claude models with sensible defaults
# KATALYST_LLM_PROFILE=ollama     # Uses local Ollama models
# KATALYST_LLM_PROFILE=groq       # Uses Groq/Llama models with sensible defaults

# Option 2: Manually specify models (advanced users)
# Use a powerful model for planning and replanning (high-reasoning tasks)
KATALYST_REASONING_MODEL="gpt-4.1"

# Use a faster, cheaper model for execution and tool use (low-reasoning tasks)
KATALYST_EXECUTION_MODEL="gpt-4.1"

# Fallback model if primary model fails
KATALYST_LLM_MODEL_FALLBACK="gpt-4o"

# Default timeout for all LLM calls
KATALYST_LLM_TIMEOUT=45

# --- Agent Behavior ---
KATALYST_AUTO_APPROVE=False
KATALYST_MAX_AGENT_CYCLES=50  # Maximum cycles for the React agent
KATALYST_RECURSION_LIMIT=250  # LangGraph recursion limit

# --- API Keys ---
# OpenAI API key (required for OpenAI provider)
OPENAI_API_KEY=

# Anthropic API key (required for Anthropic provider)
# ANTHROPIC_API_KEY=

# Groq API key (required for Groq provider)
# GROQ_API_KEY=

# Together API key (required for Together provider)
# TOGETHER_API_KEY=

# Note: Ollama runs locally and doesn't require an API key